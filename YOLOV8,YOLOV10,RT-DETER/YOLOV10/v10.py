import os
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import platform
import shutil
import tempfile
import yaml
import glob
from tqdm import tqdm
from ultralytics import YOLO
import time
import albumentations as A
from albumentations.pytorch import ToTensorV2

# =====================
# åŸºç¡€é…ç½®
# =====================
BASE_DIR = r'D:\è‚ºéƒ¨ç–¾ç—…è¯Šæ–­æ£€æµ‹æ•°æ®é›†(DST2016)\2222\lungdis(DST2016)'
WORK_DIR = r'D:\è‚ºéƒ¨ç–¾ç—…è¯Šæ–­æ£€æµ‹æ•°æ®é›†(DST2016)\2222\lungdis(DST2016)\runs'
DATASET_ROOT = BASE_DIR

# ========================
# ğŸ“ ç¡¬ç¼–ç è·¯å¾„é…ç½®åŒºåŸŸ - ç›´æ¥åœ¨è¿™é‡Œä¿®æ”¹ä½ çš„è·¯å¾„
# ========================

# ğŸ”‘ æ ¸å¿ƒè·¯å¾„é…ç½® (ä¿®æ”¹è¿™é‡Œçš„è·¯å¾„ä¸ºä½ è‡ªå·±çš„)
CONFIG = {
    # ===== æ•°æ®é›†æ ¹ç›®å½• (æ”¯æŒä¸­æ–‡è·¯å¾„) =====
    'dataset_root': DATASET_ROOT,

    # ===== YAMLé…ç½®æ–‡ä»¶è·¯å¾„ =====
    'yaml_path': f'{BASE_DIR}\\data.yaml',

    # ===== æ•°æ®é›†åˆ’åˆ†é…ç½® (å®Œæ•´ç›®å½•ç»“æ„) =====
    'dataset_splits': {
        # è®­ç»ƒé›†ç›®å½•
        'train': {
            'images': f'{BASE_DIR}\\images\\train',  # è®­ç»ƒé›†å›¾åƒç›®å½• (å®Œæ•´ç»å¯¹è·¯å¾„)
            'labels': f'{BASE_DIR}\\labels\\train'  # è®­ç»ƒé›†æ ‡ç­¾ç›®å½• (å®Œæ•´ç»å¯¹è·¯å¾„)
        },
        # éªŒè¯é›†ç›®å½•
        'val': {
            'images': f'{BASE_DIR}\\images\\valid',  # éªŒè¯é›†å›¾åƒç›®å½•
            'labels': f'{BASE_DIR}\\labels\\valid'  # éªŒè¯é›†æ ‡ç­¾ç›®å½•
        },
        # æµ‹è¯•é›†/é¢„æµ‹é›†ç›®å½• (å¯é€‰)
        'test': {
            'images': f'{BASE_DIR}\\images\\test',  # æµ‹è¯•é›†å›¾åƒç›®å½•
            'labels': f'{BASE_DIR}\\labels\\test'  # æµ‹è¯•é›†æ ‡ç­¾ç›®å½• (é¢„æµ‹æ—¶å¯ä¸æä¾›)
        }
    },

    # ===== ç±»åˆ«é…ç½® (å·²ç§»è‡³YAMLæ–‡ä»¶ï¼Œæ­¤å¤„ä»…ä½œå‚è€ƒ) =====
    # å®é™…ç±»åˆ«é…ç½®å°†ä»YAMLæ–‡ä»¶ä¸­è¯»å–ï¼Œé¿å…é‡å¤å®šä¹‰

    # ===== è®­ç»ƒå‚æ•°é…ç½® =====
    'train_params': {
        'epochs': 5,  # è®­ç»ƒè½®æ•°
        'batch_size': 16,  # æ‰¹æ¬¡å¤§å°
        'imgsz': 640,  # è¾“å…¥å›¾åƒå°ºå¯¸
        'save_dir': WORK_DIR,
        'experiment_name': 'yolov10_lung_disease'
    },

    # ===== é¢„æµ‹/æ£€æµ‹å‚æ•°é…ç½® =====
    'predict_params': {
        'source': f'{BASE_DIR}\\images\\test',  # é¢„æµ‹æº - ä¸æµ‹è¯•é›†å›¾åƒç›®å½•ç»Ÿä¸€
        'save_dir': f'{WORK_DIR}\\detect',
        'experiment_name': 'results',
        'conf_threshold': 0.25,  # ç½®ä¿¡åº¦é˜ˆå€¼
        'iou_threshold': 0.45  # IOUé˜ˆå€¼
    }
}


# ========================
# 0. å…¨å±€é…ç½®ä¸è·¯å¾„å¤„ç† (å…³é”®ä¿®å¤)
# ========================

def ensure_unicode_path(path):
    """ç¡®ä¿è·¯å¾„æ˜¯Unicodeå­—ç¬¦ä¸²ï¼Œå¤„ç†ä¸­æ–‡è·¯å¾„é—®é¢˜"""
    if isinstance(path, str):
        return path
    elif isinstance(path, Path):
        return str(path)
    return str(path)


def safe_imread(image_path):
    """å®‰å…¨è¯»å–å›¾åƒï¼Œæ”¯æŒä¸­æ–‡è·¯å¾„å’Œç‰¹æ®Šæ ¼å¼"""
    # ç¡®ä¿è·¯å¾„æ˜¯Unicode
    image_path = ensure_unicode_path(image_path)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")

    # å°è¯•ç›´æ¥è¯»å– (é€‚ç”¨äºLinux/Mac)
    try:
        img = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8), cv2.IMREAD_COLOR)
        if img is not None:
            return img
    except Exception as e:
        print(f"ç›´æ¥è¯»å–å¤±è´¥: {str(e)}")

    # Windowsç‰¹æ®Šå¤„ç†ï¼šå¤åˆ¶åˆ°ä¸´æ—¶è‹±æ–‡è·¯å¾„
    if platform.system() == 'Windows':
        print("ğŸ”„ Windowsç³»ç»Ÿæ£€æµ‹ï¼Œä½¿ç”¨ä¸´æ—¶è·¯å¾„å¤„ç†ä¸­æ–‡è·¯å¾„...")
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_dir = tempfile.mkdtemp()
            # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶å (ä¿æŒæ‰©å±•å)
            ext = os.path.splitext(image_path)[1]
            temp_path = os.path.join(temp_dir, f"temp{ext}")
            # å¤åˆ¶æ–‡ä»¶ (ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼)
            with open(image_path, 'rb') as src, open(temp_path, 'wb') as dst:
                dst.write(src.read())
            # è¯»å–ä¸´æ—¶æ–‡ä»¶
            img = cv2.imread(temp_path)
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_path)
            os.rmdir(temp_dir)
            if img is not None:
                return img
        except Exception as e:
            print(f"Windowsä¸´æ—¶è·¯å¾„å¤„ç†å¤±è´¥: {str(e)}")

    # æœ€åå°è¯•ï¼šä½¿ç”¨æ ‡å‡†imread
    try:
        img = cv2.imread(image_path)
        if img is not None:
            return img
    except Exception as e:
        print(f"æ ‡å‡†è¯»å–æ–¹å¼å¤±è´¥: {str(e)}")

    raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}. è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œæƒé™ã€‚")


def get_image_files(directory, include_roboflow=True):
    """ è·å–ç›®å½•ä¸­æ‰€æœ‰å›¾åƒæ–‡ä»¶ï¼Œæ”¯æŒç‰¹æ®Šæ ¼å¼
    å‚æ•°:
        directory: ç›®å½•è·¯å¾„
        include_roboflow: æ˜¯å¦åŒ…å«Roboflowæ ¼å¼æ–‡ä»¶ (xxx.jpg.rf.xxx.jpg)
    è¿”å›:
        æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    directory = ensure_unicode_path(directory)

    # æ”¯æŒçš„æ‰©å±•å
    extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tif', '*.tiff']

    # æ·»åŠ Roboflowæ ¼å¼æ”¯æŒ
    if include_roboflow:
        # åŒ¹é… xxx.jpg.rf.xxx.jpg å’Œç±»ä¼¼æ ¼å¼
        extensions.extend(['*.jpg.*.jpg', '*.jpeg.*.jpg', '*.png.*.jpg'])

    image_files = []
    for ext in extensions:
        # ä½¿ç”¨globæŸ¥æ‰¾æ–‡ä»¶
        pattern = os.path.join(directory, ext)
        files = glob.glob(pattern)
        image_files.extend(files)

        # é€’å½’æŸ¥æ‰¾å­ç›®å½•
        for root, _, _ in os.walk(directory):
            if root == directory:
                continue
            pattern = os.path.join(root, ext)
            files = glob.glob(pattern)
            image_files.extend(files)

    # å»é‡å¹¶æ’åº
    image_files = list(set(image_files))
    image_files.sort()

    print(f"ğŸ“ åœ¨ {directory} ä¸­æ‰¾åˆ° {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶")
    if image_files:
        print("ğŸ” æ ·æœ¬æ–‡ä»¶:")
        for i, f in enumerate(image_files[:3]):
            print(f" {i + 1}. {os.path.basename(f)}")
        if len(image_files) > 3:
            print(f" ... è¿˜æœ‰ {len(image_files) - 3} ä¸ªæ–‡ä»¶")

    return image_files


def get_label_files(directory):
    """ è·å–ç›®å½•ä¸­æ‰€æœ‰æ ‡ç­¾æ–‡ä»¶ (.txt)
    å‚æ•°:
        directory: ç›®å½•è·¯å¾„
    è¿”å›:
        æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    directory = ensure_unicode_path(directory)

    # æ”¯æŒçš„æ ‡ç­¾æ‰©å±•å
    extensions = ['*.txt']

    label_files = []
    for ext in extensions:
        # ä½¿ç”¨globæŸ¥æ‰¾æ–‡ä»¶
        pattern = os.path.join(directory, ext)
        files = glob.glob(pattern)
        label_files.extend(files)

        # é€’å½’æŸ¥æ‰¾å­ç›®å½•
        for root, _, _ in os.walk(directory):
            if root == directory:
                continue
            pattern = os.path.join(root, ext)
            files = glob.glob(pattern)
            label_files.extend(files)

    # å»é‡å¹¶æ’åº
    label_files = list(set(label_files))
    label_files.sort()

    print(f"ğŸ“ åœ¨ {directory} ä¸­æ‰¾åˆ° {len(label_files)} ä¸ªæ ‡ç­¾æ–‡ä»¶")
    if label_files:
        print("ğŸ” æ ·æœ¬æ ‡ç­¾æ–‡ä»¶:")
        for i, f in enumerate(label_files[:3]):
            print(f" {i + 1}. {os.path.basename(f)}")
        if len(label_files) > 3:
            print(f" ... è¿˜æœ‰ {len(label_files) - 3} ä¸ªæ–‡ä»¶")

    return label_files


# ========================
# 1. YOLOv10 æ ¸å¿ƒç±» (å¢å¼ºç‰ˆ)
# ========================

class YOLOv10Detector:
    """ YOLOv10ç›®æ ‡æ£€æµ‹å™¨å°è£…ç±»ï¼Œå¢å¼ºæ”¯æŒä¸­æ–‡è·¯å¾„å’Œç‰¹æ®Šæ ¼å¼ """

    def __init__(self, model_size='n', checkpoint=None, data_yaml=None):
        """ åˆå§‹åŒ–YOLOv10æ£€æµ‹å™¨
        å‚æ•°:
            model_size: æ¨¡å‹å°ºå¯¸ 'n', 's', 'm', 'b', 'l', 'x'
            checkpoint: é¢„è®­ç»ƒæƒé‡è·¯å¾„ï¼ŒNoneåˆ™ä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒæƒé‡
            data_yaml: YAMLé…ç½®æ–‡ä»¶è·¯å¾„ (å¯é€‰)
        """
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.data_yaml = data_yaml  # å­˜å‚¨YAMLé…ç½®è·¯å¾„

        print(f"ğŸš€ è®¾å¤‡: {self.device.upper()}")
        print(f"ğŸŒ ç³»ç»Ÿ: {platform.system()}, Python: {platform.python_version()}")

        # æ£€æŸ¥OpenCVç‰ˆæœ¬
        print(f"ğŸ–¼ï¸ OpenCVç‰ˆæœ¬: {cv2.__version__}")

        # æ¨¡å‹å°ºå¯¸æ˜ å°„
        size_map = {
            'n': 'yolov10n.pt',  # æœ€å°æœ€å¿«
            's': 'yolov10s.pt',  # å¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦
            'm': 'yolov10m.pt',  # ä¸­ç­‰
            'b': 'yolov10b.pt',  # åŸºç¡€
            'l': 'yolov10l.pt',  # å¤§å‹
            'x': 'yolov10x.pt'  # æœ€å¤§æœ€å‡†
        }
        model_name = size_map.get(model_size.lower(), 'yolov10s.pt')
        self.model_size = model_size

        # åŠ è½½æ¨¡å‹
        if checkpoint and Path(checkpoint).exists():
            print(f"ğŸ“¦ åŠ è½½è‡ªå®šä¹‰æƒé‡: {checkpoint}")
            checkpoint = ensure_unicode_path(checkpoint)
            self.model = YOLO(checkpoint)
            self.is_custom_model = True
        else:
            print(f"ğŸŒ åŠ è½½å®˜æ–¹é¢„è®­ç»ƒæƒé‡: {model_name}")
            # æ£€æŸ¥æƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™ä¸‹è½½
            weights_path = Path('weights') / model_name
            weights_path.parent.mkdir(exist_ok=True)
            if not weights_path.exists():
                print(f"â¬‡ï¸ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä¸‹è½½: {model_name}")
                # ä»å®˜æ–¹ä»“åº“ä¸‹è½½æƒé‡
                import urllib.request
                url = f"https://github.com/THU-MIG/yolov10/releases/download/v1.0/{model_name}"
                try:
                    urllib.request.urlretrieve(url, str(weights_path))
                    print(f"âœ… æƒé‡ä¸‹è½½æˆåŠŸ: {weights_path}")
                except Exception as e:
                    print(f"âŒ æƒé‡ä¸‹è½½å¤±è´¥: {str(e)}")
                    print("âš ï¸ å°è¯•ä½¿ç”¨å†…ç½®æ¨¡å‹åŠ è½½æ–¹å¼")
                    self.model = YOLO(model_name)
            else:
                print(f"âœ… ä½¿ç”¨æœ¬åœ°æƒé‡: {weights_path}")
                self.model = YOLO(str(weights_path))
            self.is_custom_model = False

        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.model.to(self.device)

        # åŒ»å­¦å½±åƒä¸“ç”¨é¢„å¤„ç†
        self.medical_transform = None
        self._setup_medical_transform()

        # éªŒè¯YAMLé…ç½®
        if self.data_yaml and Path(self.data_yaml).exists():
            print(f"âœ… YAMLé…ç½®æ–‡ä»¶å·²è®¾ç½®: {self.data_yaml}")
            self._validate_yaml_config()
        elif self.data_yaml:
            print(f"âš ï¸ YAMLé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_yaml}. è®­ç»ƒæ—¶éœ€è¦æä¾›æœ‰æ•ˆçš„é…ç½®ã€‚")

        print(f"âœ… YOLOv10-{model_size.upper()} åˆå§‹åŒ–å®Œæˆ!")

    def _validate_yaml_config(self):
        """éªŒè¯YAMLé…ç½®æ–‡ä»¶"""
        try:
            with open(self.data_yaml, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)

            # æ£€æŸ¥å¿…è¦å­—æ®µ
            required_fields = ['path', 'train', 'val', 'names']
            missing = [field for field in required_fields if field not in config]
            if missing:
                print(f"âš ï¸ YAMLé…ç½®ç¼ºå°‘å¿…è¦å­—æ®µ: {missing}")
                return False

            # éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨
            base_path = Path(config['path'])

            # éªŒè¯è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„imageså’Œlabelsç›®å½•
            for split in ['train', 'val']:
                if split in config:
                    # éªŒè¯imagesç›®å½•
                    img_dir = base_path / config[split]
                    if not img_dir.exists():
                        print(f"âš ï¸ {split} å›¾åƒç›®å½•ä¸å­˜åœ¨: {img_dir}")
                    else:
                        img_files = get_image_files(str(img_dir), include_roboflow=True)
                        print(f"âœ… {split} å›¾åƒç›®å½•åŒ…å« {len(img_files)} ä¸ªå›¾åƒæ–‡ä»¶")

                    # éªŒè¯labelsç›®å½• (YOLOçº¦å®šï¼šlabelsç›®å½•ä¸imagesç›®å½•åŒçº§)
                    labels_dir = base_path / str(config[split]).replace('images', 'labels')
                    if not labels_dir.exists():
                        print(f"âš ï¸ {split} æ ‡ç­¾ç›®å½•ä¸å­˜åœ¨: {labels_dir}")
                        print(f"ğŸ’¡ YOLOçº¦å®šï¼šæ ‡ç­¾ç›®å½•åº”ä¸º {labels_dir}")
                    else:
                        label_files = get_label_files(str(labels_dir))
                        print(f"âœ… {split} æ ‡ç­¾ç›®å½•åŒ…å« {len(label_files)} ä¸ªæ ‡ç­¾æ–‡ä»¶")

                    # æ£€æŸ¥å›¾åƒå’Œæ ‡ç­¾æ–‡ä»¶æ•°é‡æ˜¯å¦åŒ¹é…
                    if img_files and label_files:
                        if len(img_files) == len(label_files):
                            print(f"âœ… {split} å›¾åƒå’Œæ ‡ç­¾æ–‡ä»¶æ•°é‡åŒ¹é…: {len(img_files)}")
                        else:
                            print(
                                f"âš ï¸ {split} å›¾åƒå’Œæ ‡ç­¾æ–‡ä»¶æ•°é‡ä¸åŒ¹é…! å›¾åƒ: {len(img_files)}, æ ‡ç­¾: {len(label_files)}")

            # éªŒè¯æµ‹è¯•é›† (å¦‚æœæœ‰)
            if 'test' in config:
                test_dir = base_path / config['test']
                if test_dir.exists():
                    test_files = get_image_files(str(test_dir), include_roboflow=True)
                    print(f"âœ… æµ‹è¯•é›†ç›®å½•åŒ…å« {len(test_files)} ä¸ªå›¾åƒæ–‡ä»¶")

            # éªŒè¯ç±»åˆ«åç§°
            if 'names' in config:
                print(f"âœ… ç±»åˆ«æ•°é‡: {len(config['names'])}")
                print(f"ğŸ¯ ç±»åˆ«åç§°: {', '.join(config['names'])}")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ç±»åˆ«åç§°é…ç½®")

            print("âœ… YAMLé…ç½®éªŒè¯é€šè¿‡!")
            return True
        except Exception as e:
            print(f"âŒ YAMLé…ç½®éªŒè¯å¤±è´¥: {str(e)}")
            return False

    def _setup_medical_transform(self):
        """è®¾ç½®åŒ»å­¦å½±åƒä¸“ç”¨é¢„å¤„ç†"""
        self.medical_transform = A.Compose([
            # å¯¹æ¯”åº¦å—é™è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ– (CLAHE)
            A.CLAHE(clip_limit=3.0, tile_grid_size=(8, 8), p=0.5),
            # è°ƒæ•´å¤§å° (ä¿æŒæ¯”ä¾‹)
            A.Resize(640, 640, always_apply=True),
            # å½’ä¸€åŒ– (å•é€šé“åŒ»å­¦å½±åƒç»Ÿè®¡)
            A.Normalize(mean=[0.5], std=[0.25], max_pixel_value=255.0),
            ToTensorV2()
        ])

    def train(self, epochs=100, batch_size=16, imgsz=640, patience=10,
              save_dir='runs/train', name='exp', exist_ok=True, resume=False, **kwargs):
        """ è®­ç»ƒYOLOv10æ¨¡å‹
        å‚æ•°:
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            imgsz: è¾“å…¥å›¾åƒå°ºå¯¸
            patience: æ—©åœè€å¿ƒå€¼
            save_dir: ä¿å­˜ç›®å½•
            name: å®éªŒåç§°
            exist_ok: æ˜¯å¦è¦†ç›–ç°æœ‰å®éªŒ
            resume: æ˜¯å¦ä»ä¸Šæ¬¡ä¸­æ–­å¤„æ¢å¤
            **kwargs: å…¶ä»–è®­ç»ƒå‚æ•°
        è¿”å›:
            è®­ç»ƒç»“æœ
        """
        if not self.data_yaml or not Path(self.data_yaml).exists():
            raise ValueError("è®­ç»ƒéœ€è¦æœ‰æ•ˆçš„YAMLé…ç½®æ–‡ä»¶ã€‚è¯·åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®data_yamlå‚æ•°ã€‚")

        print("=" * 50)
        print("ğŸ¯ å¼€å§‹è®­ç»ƒ YOLOv10")
        print("=" * 50)
        print(f"ğŸ“Š é…ç½®:")
        print(f"  YAMLé…ç½®: {self.data_yaml}")
        print(f"  è®­ç»ƒè½®æ•°: {epochs}")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  å›¾åƒå°ºå¯¸: {imgsz}")
        print(f"  ä¿å­˜ç›®å½•: {save_dir}/{name}")

        # è®­ç»ƒå‚æ•°
        train_args = {
            'data': ensure_unicode_path(self.data_yaml),
            'epochs': epochs,
            'batch': batch_size,
            'imgsz': imgsz,
            'device': self.device,
            'project': save_dir,
            'name': name,
            'exist_ok': exist_ok,
            'patience': patience,
            'resume': resume,
            'verbose': True,
            'workers': 4,
            'cache': False,  # ç¦ç”¨ç¼“å­˜ï¼Œé¿å…ä¸­æ–‡è·¯å¾„é—®é¢˜
            'close_mosaic': 10,  # æœ€å10è½®å…³é—­mosaicå¢å¼º
            **kwargs
        }

        # æ‰§è¡Œè®­ç»ƒ
        try:
            results = self.model.train(**train_args)
            print("âœ… è®­ç»ƒå®Œæˆ!")
            return results
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}")

            # æä¾›è¯¦ç»†çš„é”™è¯¯è¯Šæ–­
            if "does not exist" in str(e).lower() or "not found" in str(e).lower():
                print("ğŸ” è¯Šæ–­: å¯èƒ½æ˜¯è·¯å¾„é—®é¢˜ã€‚æ£€æŸ¥ä»¥ä¸‹å†…å®¹:")
                print(f" - YAMLæ–‡ä»¶è·¯å¾„: {self.data_yaml}")
                print(f" - è®­ç»ƒé›†/éªŒè¯é›†ç›®å½•æ˜¯å¦å­˜åœ¨äºYAMLæŒ‡å®šçš„ä½ç½®")
                print(f" - æ ‡ç­¾ç›®å½•æ˜¯å¦ä¸å›¾åƒç›®å½•ç»“æ„åŒ¹é…")
                print(f" - ç¡®ä¿æ‰€æœ‰è·¯å¾„ä¸åŒ…å«ç‰¹æ®Šå­—ç¬¦æˆ–è¿‡é•¿çš„è·¯å¾„å")

            if "cannot identify image file" in str(e).lower():
                print("ğŸ” è¯Šæ–­: å›¾åƒæ–‡ä»¶è¯†åˆ«é—®é¢˜ã€‚æ£€æŸ¥ä»¥ä¸‹å†…å®¹:")
                print(f" - ç¡®ä¿æ‰€æœ‰å›¾åƒæ–‡ä»¶å®Œæ•´ä¸”æ ¼å¼æ­£ç¡®")
                print(f" - å¯¹äºRoboflowæ ¼å¼æ–‡ä»¶ (xxx.jpg.rf.xxx.jpg)ï¼Œç¡®ä¿æ–‡ä»¶æ‰©å±•åæ­£ç¡®")
                print(f" - ç¡®ä¿æ¯ä¸ªå›¾åƒéƒ½æœ‰å¯¹åº”çš„.txtæ ‡ç­¾æ–‡ä»¶")
                print(f" - å°è¯•å°†æ•°æ®é›†ç§»åŠ¨åˆ°ç®€å•è‹±æ–‡è·¯å¾„")

            raise

    def detect(self, source, conf_threshold=0.25, iou_threshold=0.45,
               save=False, save_path=None, visualize=True):
        """ æ‰§è¡Œç›®æ ‡æ£€æµ‹ï¼Œå¢å¼ºä¸­æ–‡è·¯å¾„æ”¯æŒ """
        # ç¡®ä¿sourceè·¯å¾„æ­£ç¡®
        source = ensure_unicode_path(source)
        print(f"ğŸ” å¼€å§‹æ£€æµ‹: {source}")

        # ç‰¹æ®Šå¤„ç†ä¸­æ–‡è·¯å¾„
        temp_source = None
        if platform.system() == 'Windows' and any('ä¸€' <= c <= 'é¿¿' for c in source):
            print("ğŸ”„ Windowsç³»ç»Ÿæ£€æµ‹åˆ°ä¸­æ–‡è·¯å¾„ï¼Œä½¿ç”¨ä¸´æ—¶è·¯å¾„å¤„ç†...")
            try:
                # æ£€æŸ¥æ˜¯æ–‡ä»¶è¿˜æ˜¯ç›®å½•
                if os.path.isfile(source):
                    # åˆ›å»ºä¸´æ—¶ç›®å½•
                    temp_dir = tempfile.mkdtemp()
                    # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶å
                    ext = os.path.splitext(source)[1]
                    temp_source = os.path.join(temp_dir, f"temp{ext}")
                    # å¤åˆ¶æ–‡ä»¶
                    with open(source, 'rb') as src, open(temp_source, 'wb') as dst:
                        dst.write(src.read())
                elif os.path.isdir(source):
                    # å¯¹äºç›®å½•ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹è·¯å¾„ï¼Œä½†æé†’ç”¨æˆ·
                    print("âš ï¸ Windowsç³»ç»Ÿä¸Šä¸­æ–‡ç›®å½•è·¯å¾„å¯èƒ½å­˜åœ¨é—®é¢˜ã€‚å»ºè®®å°†æ•°æ®é›†ç§»åŠ¨åˆ°è‹±æ–‡è·¯å¾„ã€‚")
                    temp_source = source
                else:
                    temp_source = source
            except Exception as e:
                print(f"âš ï¸ ä¸´æ—¶è·¯å¾„åˆ›å»ºå¤±è´¥: {str(e)}ã€‚å°è¯•ç›´æ¥ä½¿ç”¨åŸå§‹è·¯å¾„ã€‚")
                temp_source = source
        else:
            temp_source = source

        start_time = time.time()

        # æ¨ç†å‚æ•°
        args = {
            'conf': conf_threshold,
            'iou': iou_threshold,
            'imgsz': 640,
            'device': self.device,
            'save': save,
            'project': 'runs/detect',
            'name': 'exp',
            'exist_ok': True,
            'half': True if self.device == 'cuda' else False,
            'show': visualize and not save
        }

        # æ‰§è¡Œæ£€æµ‹
        try:
            results = self.model.predict(source=temp_source, **args)
        except Exception as e:
            print(f"âŒ æ£€æµ‹å¤±è´¥: {str(e)}")
            if temp_source != source:
                print(f"ğŸ” åŸå§‹è·¯å¾„: {source}")
                print(f"ğŸ” ä¸´æ—¶è·¯å¾„: {temp_source}")
            raise

        # å¤„ç†ç»“æœ
        inference_time = time.time() - start_time
        fps = 1.0 / inference_time if inference_time > 0 else 0
        print(f"âœ… æ£€æµ‹å®Œæˆ! è€—æ—¶: {inference_time:.3f}s ({fps:.1f} FPS)")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_source != source and temp_source and os.path.exists(temp_source):
            try:
                if os.path.isfile(temp_source):
                    os.unlink(temp_source)
                elif os.path.isdir(temp_source) and 'temp' in temp_source.lower():
                    shutil.rmtree(temp_source)
            except Exception as e:
                print(f"âš ï¸ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å¤±è´¥: {str(e)}")

        # å¤„ç†ç»“æœä¿å­˜
        if save and save_path:
            # ç¡®ä¿ä¿å­˜è·¯å¾„æ˜¯Unicode
            save_path = ensure_unicode_path(save_path)
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            Path(os.path.dirname(save_path)).mkdir(parents=True, exist_ok=True)

            # å¤„ç†ç»“æœæ–‡ä»¶
            result_dir = Path(args['project']) / args['name']
            if result_dir.exists():
                try:
                    # è·å–æœ€æ–°ç»“æœæ–‡ä»¶
                    result_files = list(result_dir.glob('*.*'))
                    if result_files:
                        latest_file = max(result_files, key=os.path.getctime)
                        # å¤åˆ¶åˆ°ç›®æ ‡ä½ç½®
                        shutil.copy2(str(latest_file), save_path)
                        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
                except Exception as e:
                    print(f"âš ï¸ ç»“æœä¿å­˜å¤±è´¥: {str(e)}")

        return results


# ========================
# 2. æ•°æ®é›†é…ç½®å·¥å…· (å®Œæ•´ç›®å½•ç»“æ„ç‰ˆ)
# ========================

class DatasetConfigurator:
    """æ•°æ®é›†é…ç½®å·¥å…·ï¼Œæ”¯æŒå®Œæ•´ç›®å½•ç»“æ„å’Œä¸­æ–‡è·¯å¾„"""

    def __init__(self, config):
        """ åˆå§‹åŒ–æ•°æ®é›†é…ç½®
        å‚æ•°:
            config: åŒ…å«æ‰€æœ‰è·¯å¾„é…ç½®çš„å­—å…¸
        """
        self.config = config
        self.dataset_root = ensure_unicode_path(config['dataset_root'])

        # æ„å»ºå®Œæ•´è·¯å¾„
        self.paths = {}
        for split_name, split_config in config['dataset_splits'].items():
            self.paths[split_name] = {
                'images': split_config['images'] if split_config.get('images') else None,
                'labels': split_config['labels'] if split_config.get('labels') else None
            }

        # éªŒè¯ç›®å½•ç»“æ„
        self._validate_dataset_structure()

    def _validate_dataset_structure(self):
        """éªŒè¯å®Œæ•´çš„æ•°æ®é›†ç›®å½•ç»“æ„"""
        print("\n" + "=" * 50)
        print("ğŸ” éªŒè¯æ•°æ®é›†ç›®å½•ç»“æ„")
        print("=" * 50)

        # éªŒè¯æ ¹ç›®å½•
        if not Path(self.dataset_root).exists():
            raise ValueError(f"âŒ æ•°æ®é›†æ ¹ç›®å½•ä¸å­˜åœ¨: {self.dataset_root}")
        print(f"âœ… æ•°æ®é›†æ ¹ç›®å½•: {self.dataset_root}")

        # éªŒè¯æ‰€æœ‰ç›®å½•
        for split in ['train', 'val', 'test']:
            if split not in self.paths:
                continue

            print(f"\nğŸ“Š {split.upper()} æ•°æ®é›†éªŒè¯:")

            # éªŒè¯imagesç›®å½•
            img_dir = self.paths[split]['images']
            if img_dir and Path(img_dir).exists():
                img_files = get_image_files(img_dir, include_roboflow=True)
                print(f"âœ… {split} å›¾åƒç›®å½•: {img_dir}")
                print(f" ğŸ“ å›¾åƒæ–‡ä»¶æ•°é‡: {len(img_files)}")
                # æ˜¾ç¤ºéƒ¨åˆ†æ ·æœ¬
                if img_files:
                    print(" ğŸ–¼ï¸ æ ·æœ¬å›¾åƒ:")
                    for i, f in enumerate(img_files[:3]):
                        print(f"  {i + 1}. {os.path.basename(f)}")
            elif img_dir:
                print(f"âŒ {split} å›¾åƒç›®å½•ä¸å­˜åœ¨: {img_dir}")

            # éªŒè¯labelsç›®å½• (è®­ç»ƒå’ŒéªŒè¯éœ€è¦ï¼Œæµ‹è¯•å¯é€‰)
            label_dir = self.paths[split]['labels']
            if label_dir and Path(label_dir).exists():
                label_files = get_label_files(label_dir)
                print(f"âœ… {split} æ ‡ç­¾ç›®å½•: {label_dir}")
                print(f" ğŸ“„ æ ‡ç­¾æ–‡ä»¶æ•°é‡: {len(label_files)}")
                # æ˜¾ç¤ºéƒ¨åˆ†æ ·æœ¬
                if label_files:
                    print(" ğŸ·ï¸ æ ·æœ¬æ ‡ç­¾:")
                    for i, f in enumerate(label_files[:3]):
                        print(f"  {i + 1}. {os.path.basename(f)}")
            elif label_dir and split in ['train', 'val']:
                # è®­ç»ƒå’ŒéªŒè¯å¿…é¡»æœ‰æ ‡ç­¾
                print(f"âŒ {split} æ ‡ç­¾ç›®å½•ä¸å­˜åœ¨: {label_dir}")
            elif label_dir:
                print(f"âš ï¸ {split} æ ‡ç­¾ç›®å½•ä¸å­˜åœ¨ (é¢„æµ‹æ—¶å¯ä¸æä¾›): {label_dir}")

    def create_yaml_config(self, override=True):
        """åˆ›å»ºYAMLé…ç½®æ–‡ä»¶ï¼ŒåŒ…å«å®Œæ•´çš„ç›®å½•ç»“æ„"""
        yaml_path = Path(self.config['yaml_path'])
        if yaml_path.exists() and not override:
            print(f"âš ï¸ YAMLé…ç½®æ–‡ä»¶å·²å­˜åœ¨: {yaml_path}")
            print("ğŸ”„ ä½¿ç”¨ç°æœ‰é…ç½®æ–‡ä»¶")
            return str(yaml_path)

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        yaml_path.parent.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºé…ç½®
        config = {
            'path': str(self.dataset_root),  # æ•°æ®é›†æ ¹ç›®å½•
            'train': str(Path(self.config['dataset_splits']['train']['images']).relative_to(self.dataset_root)),
            # è®­ç»ƒé›†å›¾åƒç›®å½• (ç›¸å¯¹è·¯å¾„)
            'val': str(Path(self.config['dataset_splits']['val']['images']).relative_to(self.dataset_root)),
            # éªŒè¯é›†å›¾åƒç›®å½• (ç›¸å¯¹è·¯å¾„)
            'names': {
                0: "atelectasis",
                1: "cardiomegaly",
                2: "consolidation",
                3: "edema",
                4: "effusion",
                5: "emphysema",
                6: "fibrosis",
                7: "hernia",
                8: "infiltration",
                9: "mass",
                10: "nodule",
                11: "pleural_thickening",
                12: "pneumonia",
                13: "pneumothorax"
            },
            'nc': 14
        }

        # æ·»åŠ æµ‹è¯•é›† (å¦‚æœæœ‰)
        if 'test' in self.config['dataset_splits'] and self.config['dataset_splits']['test']['images']:
            config['test'] = str(Path(self.config['dataset_splits']['test']['images']).relative_to(self.dataset_root))

        # ä¿å­˜YAML
        try:
            with open(yaml_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, sort_keys=False, allow_unicode=True)
            print(f"\n" + "=" * 50)
            print(f"âœ… YAMLé…ç½®æ–‡ä»¶å·²åˆ›å»º: {yaml_path}")
            print("=" * 50)
            print("ğŸ“ YAMLé…ç½®å†…å®¹:")
            print(f"  path: {config['path']}")
            print(f"  train: {config['train']}")
            print(f"  val: {config['val']}")
            if 'test' in config:
                print(f"  test: {config['test']}")
            print(f"  nc: {config['nc']}")
            print(f"  names: {config['names']}")
            return str(yaml_path)
        except Exception as e:
            print(f"âŒ ä¿å­˜YAMLé…ç½®å¤±è´¥: {str(e)}")
            raise

    def verify_file_matching(self):
        """éªŒè¯å›¾åƒæ–‡ä»¶å’Œæ ‡ç­¾æ–‡ä»¶æ˜¯å¦åŒ¹é…"""
        print("\n" + "=" * 50)
        print("ğŸ” éªŒè¯æ–‡ä»¶åŒ¹é…æ€§")
        print("=" * 50)

        for split in ['train', 'val']:
            if split not in self.paths:
                continue

            img_dir = self.paths[split]['images']
            label_dir = self.paths[split]['labels']

            if not (img_dir and label_dir and Path(img_dir).exists() and Path(label_dir).exists()):
                continue

            print(f"\nğŸ“Š {split.upper()} æ•°æ®é›†æ–‡ä»¶åŒ¹é…éªŒè¯:")

            # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶ (ä¸åŒ…æ‹¬è·¯å¾„ï¼Œåªå–æ–‡ä»¶å)
            img_files = get_image_files(img_dir, include_roboflow=True)
            img_names = {os.path.splitext(os.path.basename(f))[0]: f for f in img_files}

            # è·å–æ‰€æœ‰æ ‡ç­¾æ–‡ä»¶
            label_files = get_label_files(label_dir)
            label_names = {os.path.splitext(os.path.basename(f))[0]: f for f in label_files}

            # æ‰¾å‡ºæœ‰å›¾åƒä½†æ²¡æœ‰æ ‡ç­¾çš„æ–‡ä»¶
            missing_labels = [name for name in img_names if name not in label_names]

            # æ‰¾å‡ºæœ‰æ ‡ç­¾ä½†æ²¡æœ‰å›¾åƒçš„æ–‡ä»¶
            missing_images = [name for name in label_names if name not in img_names]

            print(f"âœ… æ€»å›¾åƒæ–‡ä»¶: {len(img_files)}")
            print(f"âœ… æ€»æ ‡ç­¾æ–‡ä»¶: {len(label_files)}")
            print(f"âœ… åŒ¹é…æ–‡ä»¶æ•°: {len(img_names) - len(missing_labels)}")

            if missing_labels:
                print(f"âŒ {len(missing_labels)} ä¸ªå›¾åƒç¼ºå°‘æ ‡ç­¾æ–‡ä»¶:")
                for i, name in enumerate(missing_labels[:5]):
                    print(f"  {i + 1}. {name} (å›¾åƒ: {img_names[name]})")
                if len(missing_labels) > 5:
                    print(f"  ... è¿˜æœ‰ {len(missing_labels) - 5} ä¸ª")

            if missing_images:
                print(f"âŒ {len(missing_images)} ä¸ªæ ‡ç­¾ç¼ºå°‘å›¾åƒæ–‡ä»¶:")
                for i, name in enumerate(missing_images[:5]):
                    print(f"  {i + 1}. {name} (æ ‡ç­¾: {label_names[name]})")
                if len(missing_images) > 5:
                    print(f"  ... è¿˜æœ‰ {len(missing_images) - 5} ä¸ª")

            if not missing_labels and not missing_images:
                print("âœ… æ‰€æœ‰å›¾åƒå’Œæ ‡ç­¾æ–‡ä»¶å®Œç¾åŒ¹é…!")


# ========================
# 3. ä¸»ç¨‹åº (å®Œæ•´ç›®å½•ç»“æ„ç‰ˆ)
# ========================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ YOLOv10 è®­ç»ƒä¸é¢„æµ‹ç³»ç»Ÿ (ä¿®æ­£å®Œæ•´ç‰ˆ)")
    print("=" * 60)

    # ========================
    # 1. æ˜¾ç¤ºå½“å‰é…ç½®
    # ========================
    print("\n" + "=" * 50)
    print("ğŸ“‹ å½“å‰è·¯å¾„é…ç½®")
    print("=" * 50)
    print(f"ğŸ“ æ•°æ®é›†æ ¹ç›®å½•: {CONFIG['dataset_root']}")

    # è®­ç»ƒé›†é…ç½®
    print(f"\nğŸ‹ï¸ è®­ç»ƒé›†é…ç½®:")
    train_img_dir = CONFIG['dataset_splits']['train']['images']
    train_label_dir = CONFIG['dataset_splits']['train']['labels']
    print(f" ğŸ–¼ï¸ å›¾åƒç›®å½•: {train_img_dir}")
    print(f" ğŸ·ï¸ æ ‡ç­¾ç›®å½•: {train_label_dir}")

    # éªŒè¯é›†é…ç½®
    print(f"\nğŸ“Š éªŒè¯é›†é…ç½®:")
    val_img_dir = CONFIG['dataset_splits']['val']['images']
    val_label_dir = CONFIG['dataset_splits']['val']['labels']
    print(f" ğŸ–¼ï¸ å›¾åƒç›®å½•: {val_img_dir}")
    print(f" ğŸ·ï¸ æ ‡ç­¾ç›®å½•: {val_label_dir}")

    # æµ‹è¯•é›†é…ç½®
    print(f"\nğŸ” æµ‹è¯•é›†é…ç½®:")
    if 'test' in CONFIG['dataset_splits'] and CONFIG['dataset_splits']['test']['images']:
        test_img_dir = CONFIG['dataset_splits']['test']['images']
        print(f" ğŸ–¼ï¸ å›¾åƒç›®å½•: {test_img_dir}")
    if 'test' in CONFIG['dataset_splits'] and CONFIG['dataset_splits']['test'].get('labels'):
        test_label_dir = CONFIG['dataset_splits']['test']['labels']
        print(f" ğŸ·ï¸ æ ‡ç­¾ç›®å½•: {test_label_dir}")
    if not ('test' in CONFIG['dataset_splits'] and (
            CONFIG['dataset_splits']['test']['images'] or CONFIG['dataset_splits']['test'].get('labels'))):
        print(" âš ï¸ æœªé…ç½®æµ‹è¯•é›†")

    # è®­ç»ƒå‚æ•°
    print(f"\nâš™ï¸ è®­ç»ƒå‚æ•°:")
    print(f" ğŸ” è®­ç»ƒè½®æ•°: {CONFIG['train_params']['epochs']}")
    print(f" ğŸ“¦ æ‰¹æ¬¡å¤§å°: {CONFIG['train_params']['batch_size']}")
    print(f" ğŸ–¼ï¸ å›¾åƒå°ºå¯¸: {CONFIG['train_params']['imgsz']}")
    print(f" ğŸ’¾ ä¿å­˜ç›®å½•: {CONFIG['train_params']['save_dir']}/{CONFIG['train_params']['experiment_name']}")

    # é¢„æµ‹å‚æ•°
    print(f"\nğŸ”® é¢„æµ‹å‚æ•°:")
    print(f" ğŸ“‚ é¢„æµ‹æº: {CONFIG['predict_params']['source']}")
    print(f" ğŸ’¾ ä¿å­˜ç›®å½•: {CONFIG['predict_params']['save_dir']}/{CONFIG['predict_params']['experiment_name']}")
    print(f" âœ… ç½®ä¿¡åº¦é˜ˆå€¼: {CONFIG['predict_params']['conf_threshold']}")
    print(f" ğŸ¯ IOUé˜ˆå€¼: {CONFIG['predict_params']['iou_threshold']}")

    # ========================
    # 2. éªŒè¯å’Œåˆ›å»ºYAMLé…ç½®
    # ========================
    print("\n" + "=" * 50)
    print("âš™ï¸ éªŒè¯æ•°æ®é›†å’Œåˆ›å»ºYAMLé…ç½®")
    print("=" * 50)

    try:
        # åˆ›å»ºé…ç½®å™¨
        configurator = DatasetConfigurator(CONFIG)

        # åˆ›å»ºYAMLé…ç½®
        yaml_path = configurator.create_yaml_config(override=True)

        # éªŒè¯æ–‡ä»¶åŒ¹é…æ€§
        configurator.verify_file_matching()
    except Exception as e:
        print(f"âŒ é…ç½®åˆ›å»ºå¤±è´¥: {str(e)}")
        print("ğŸ’¡ è¯·æ£€æŸ¥ä»¥ä¸‹å†…å®¹:")
        print(f" - æ•°æ®é›†æ ¹ç›®å½•æ˜¯å¦å­˜åœ¨: {CONFIG['dataset_root']}")
        print(f" - è®­ç»ƒé›†å›¾åƒç›®å½•æ˜¯å¦å­˜åœ¨: {train_img_dir}")
        print(f" - è®­ç»ƒé›†æ ‡ç­¾ç›®å½•æ˜¯å¦å­˜åœ¨: {train_label_dir}")
        print(f" - éªŒè¯é›†å›¾åƒç›®å½•æ˜¯å¦å­˜åœ¨: {val_img_dir}")
        print(f" - éªŒè¯é›†æ ‡ç­¾ç›®å½•æ˜¯å¦å­˜åœ¨: {val_label_dir}")
        exit(1)

    # ========================
    # 3. è®­ç»ƒæ¨¡å‹
    # ========================
    print("\n" + "=" * 50)
    print("ğŸ‹ï¸ æ¨¡å‹è®­ç»ƒ")
    print("=" * 50)

    try:
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        detector = YOLOv10Detector(
            model_size='s',  # ä½¿ç”¨å°å‹æ¨¡å‹
            checkpoint=None,
            data_yaml=yaml_path
        )

        # å¼€å§‹è®­ç»ƒ
        results = detector.train(
            epochs=CONFIG['train_params']['epochs'],
            batch_size=CONFIG['train_params']['batch_size'],
            imgsz=CONFIG['train_params']['imgsz'],
            patience=15,
            save_dir=CONFIG['train_params']['save_dir'],
            name=CONFIG['train_params']['experiment_name'],
            exist_ok=True,
            resume=False
        )
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}")
        print("ğŸ’¡ è¯·æ£€æŸ¥ä»¥ä¸‹å†…å®¹:")
        print(f" - YAMLé…ç½®æ–‡ä»¶æ˜¯å¦æ­£ç¡®: {yaml_path}")
        print(f" - è®­ç»ƒé›†/éªŒè¯é›†è·¯å¾„æ˜¯å¦åœ¨YAMLä¸­æ­£ç¡®é…ç½®")
        print(f" - å›¾åƒå’Œæ ‡ç­¾æ–‡ä»¶æ˜¯å¦åŒ¹é…")
        print(f" - GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿ (å°è¯•å‡å°batch_size)")
        # ä¸é€€å‡ºï¼Œç»§ç»­å°è¯•é¢„æµ‹

    # ========================
    # 4. é¢„æµ‹/æ£€æµ‹
    # ========================
    print("\n" + "=" * 50)
    print("ğŸ” æ¨¡å‹é¢„æµ‹/æ£€æµ‹")
    print("=" * 50)

    try:
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        if 'detector' not in locals():
            detector = YOLOv10Detector(
                model_size='s',
                checkpoint=None,
                data_yaml=yaml_path
            )

        # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æƒé‡
        best_weight_path = Path(CONFIG['train_params']['save_dir']) / CONFIG['train_params'][
            'experiment_name'] / 'weights' / 'best.pt'
        if best_weight_path.exists():
            print(f"âœ… æ‰¾åˆ°è®­ç»ƒå¥½çš„æœ€ä½³æƒé‡: {best_weight_path}")
            detector = YOLOv10Detector(
                model_size='s',
                checkpoint=str(best_weight_path),  # ä½¿ç”¨è®­ç»ƒå¥½çš„æƒé‡
                data_yaml=yaml_path
            )
        else:
            print("âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æƒé‡ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹")

        # åˆ›å»ºé¢„æµ‹ä¿å­˜ç›®å½•
        predict_save_dir = Path(CONFIG['predict_params']['save_dir']) / CONFIG['predict_params']['experiment_name']
        predict_save_dir.mkdir(parents=True, exist_ok=True)

        # ä¸ºæ¯ä¸ªæµ‹è¯•å›¾åƒåˆ›å»ºå•ç‹¬çš„ä¿å­˜è·¯å¾„
        predict_source = CONFIG['predict_params']['source']

        # ç¡®å®šé¢„æµ‹æºæ˜¯ç›®å½•è¿˜æ˜¯æ–‡ä»¶
        if os.path.isdir(predict_source):
            # è·å–æ‰€æœ‰æµ‹è¯•å›¾åƒ
            test_images = get_image_files(predict_source, include_roboflow=True)
            print(f"ğŸ–¼ï¸ æ‰¾åˆ° {len(test_images)} ä¸ªæµ‹è¯•å›¾åƒ")

            # é€ä¸€é¢„æµ‹
            for i, img_path in enumerate(test_images[:5]):  # åªé¢„æµ‹å‰5ä¸ªä½œä¸ºç¤ºä¾‹
                img_name = os.path.basename(img_path)
                save_path = predict_save_dir / f"result_{i + 1}_{img_name}"
                print(f"\nğŸ” é¢„æµ‹å›¾åƒ {i + 1}/{len(test_images[:5])}: {img_name}")
                print(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ°: {save_path}")

                detector.detect(
                    source=img_path,
                    conf_threshold=CONFIG['predict_params']['conf_threshold'],
                    iou_threshold=CONFIG['predict_params']['iou_threshold'],
                    save=True,
                    save_path=str(save_path),
                    visualize=False
                )

            if len(test_images) > 5:
                print(f"\nğŸ’¡ ä»…é¢„æµ‹äº†å‰5ä¸ªå›¾åƒä½œä¸ºç¤ºä¾‹ï¼Œæ€»å…± {len(test_images)} ä¸ªæµ‹è¯•å›¾åƒ")
        else:
            # å•ä¸ªæ–‡ä»¶é¢„æµ‹
            save_path = predict_save_dir / f"result_{os.path.basename(predict_source)}"
            print(f"ğŸ” é¢„æµ‹å•ä¸ªæ–‡ä»¶: {predict_source}")
            print(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ°: {save_path}")

            detector.detect(
                source=predict_source,
                conf_threshold=CONFIG['predict_params']['conf_threshold'],
                iou_threshold=CONFIG['predict_params']['iou_threshold'],
                save=True,
                save_path=str(save_path),
                visualize=True
            )

        print("\nâœ… é¢„æµ‹å®Œæˆ!")
        print(f"ğŸ“ æ‰€æœ‰é¢„æµ‹ç»“æœä¿å­˜åœ¨: {predict_save_dir}")
    except Exception as e:
        print(f"âŒ é¢„æµ‹å¤±è´¥: {str(e)}")
        print("ğŸ’¡ è¯·æ£€æŸ¥ä»¥ä¸‹å†…å®¹:")
        print(f" - é¢„æµ‹æºè·¯å¾„æ˜¯å¦å­˜åœ¨: {predict_source}")
        print(f" - æ¨¡å‹æƒé‡æ˜¯å¦æ­£ç¡®åŠ è½½")
        print(f" - é¢„æµ‹ä¿å­˜ç›®å½•æ˜¯å¦æœ‰å†™å…¥æƒé™: {CONFIG['predict_params']['save_dir']}")

    print("\n" + "=" * 60)
    print("ğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆ!")
    print("=" * 60)