# RNTR.py
"""
å…¨è‡ªåŠ¨è‚ºéƒ¨CTç—…ç¶æ£€æµ‹ç³»ç»Ÿ (RT-DETR)
- æ”¯æŒä¸­æ–‡è·¯å¾„ & .rf. æ–‡ä»¶å
- è‡ªåŠ¨è®­ç»ƒ + æ‰¹é‡é¢„æµ‹ + è¯„ä¼° + å¯è§†åŒ–
- è¿è¡Œå³å…¨è‡ªåŠ¨æ‰§è¡Œï¼Œæ— éœ€å‘½ä»¤è¡Œå‚æ•°
"""

import os
import sys
import shutil
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from ultralytics import YOLO
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import roc_curve, auc
import torch
import warnings

# ==================== ã€å…³é”®ä¿®å¤ã€‘====================
torch.use_deterministic_algorithms(False)  # å…è®¸éç¡®å®šæ€§ç®—æ³•
warnings.filterwarnings("ignore", category=UserWarning, module="torch")  # å¿½ç•¥ç›¸å…³è­¦å‘Š
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # ç¨³å®š CUDA æ“ä½œ
# =====================================================
# ==================== ã€ç”¨æˆ·é…ç½®åŒºã€‘====================
ROOT_DIR = r"D:\2016\2222\lungdis(DST2016)"      # æ•°æ®æ ¹ç›®å½•
RESULTS_DIR = r"D:\2016\2222\lungdis(DST2016)"    # ç»“æœè¾“å‡ºç›®å½•

MODEL_NAME = "rtdetr-l.pt"
IMG_SIZE = 1024
BATCH_SIZE = 4
EPOCHS = 50
DEVICE = 0  # -1 for CPU

CLASS_NAMES = [
    "atelectasis", "cardiomegaly", "consolidation", "edema", "effusion",
    "emphysema", "fibrosis", "hernia", "infiltration", "mass",
    "nodule", "pleural_thickening", "pneumonia", "pneumothorax"
]
NUM_CLASSES = len(CLASS_NAMES)

# =====================================================

def ensure_path(path):
    p = Path(path)
    p.mkdir(parents=True, exist_ok=True)
    return str(p.resolve())

def find_label_file(image_path):
    img_path = Path(image_path)
    stem = img_path.stem
    label_path = img_path.parent.parent.parent / "labels" / img_path.parent.name / f"{stem}.txt"
    return label_path if label_path.exists() else None

def prepare_yolo_dataset(images_dir, output_dir):
    images_dir = Path(images_dir)
    output_dir = Path(output_dir)
    out_images = output_dir / "images"
    out_labels = output_dir / "labels"
    out_images.mkdir(parents=True, exist_ok=True)
    out_labels.mkdir(parents=True, exist_ok=True)

    image_files = []
    for ext in ["*.jpg", "*.jpeg", "*.png", "*.JPG", "*.PNG"]:
        image_files.extend(images_dir.rglob(ext))

    copied = 0
    for img_path in image_files:
        label_path = find_label_file(img_path)
        if not label_path:
            continue

        new_img_name = f"{copied:06d}.jpg"
        new_img_path = out_images / new_img_name
        new_label_path = out_labels / f"{copied:06d}.txt"

        img = cv2.imread(str(img_path))
        if img is None:
            continue
        if img.shape[0] != IMG_SIZE or img.shape[1] != IMG_SIZE:
            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
        cv2.imwrite(str(new_img_path), img)
        shutil.copy(label_path, new_label_path)
        copied += 1

    print(f"âœ… å·²å¤„ç† {copied} å¼ å›¾åƒåˆ° {output_dir}")
    return str(output_dir)

import yaml

import yaml

def create_data_yaml(train_dir):
    """
    åˆ›å»º YOLO æ ¼å¼çš„ data.yaml æ–‡ä»¶å¹¶è¿”å›å…¶è·¯å¾„
    """
    data_dict = {
        'path': str(Path(train_dir).parent.parent),  # æŒ‡å‘ lungdis(DST2016) æ ¹ç›®å½•
        'train': 'images/train',
        'val': 'images/val',
        'names': CLASS_NAMES  # Ultralytics æ”¯æŒç›´æ¥ list
    }

    yaml_path = os.path.join(RESULTS_DIR, "data.yaml")
    Path(yaml_path).parent.mkdir(parents=True, exist_ok=True)

    with open(yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(data_dict, f, allow_unicode=True, default_flow_style=False)

    print(f"âœ… data.yaml å·²ä¿å­˜è‡³: {yaml_path}")
    return yaml_path


def check_dataset_integrity():
    from pathlib import Path
    root = Path(ROOT_DIR)
    for split in ['train', 'val']:
        img_dir = root / 'images' / split
        lbl_dir = root / 'labels' / split

        # è·å–æ‰€æœ‰å›¾åƒ
        imgs = []
        for ext in ['*.jpg', '*.jpeg', '*.png']:
            imgs.extend(img_dir.rglob(ext))

        # æ£€æŸ¥å¯¹åº”æ ‡ç­¾
        valid_imgs = []
        for img in imgs:
            label_path = lbl_dir / f"{img.stem}.txt"
            if label_path.exists():
                # æ£€æŸ¥æ ‡ç­¾æ–‡ä»¶æ˜¯å¦éç©º
                if os.path.getsize(label_path) > 0:
                    valid_imgs.append(img)

        print(f"\nğŸ” {split.upper()} é›†è¯Šæ–­:")
        print(f"   â€¢ æ‰«æç›®å½•: {img_dir}")
        print(f"   â€¢ æ‰¾åˆ°å›¾åƒ: {len(imgs)}")
        print(f"   â€¢ æœ‰æ•ˆå›¾åƒ (æœ‰éç©ºæ ‡ç­¾): {len(valid_imgs)}")
        print(f"   â€¢ æ ‡ç­¾ç¤ºä¾‹: {list(lbl_dir.glob('*.txt'))[:3]}")

    # æ£€æŸ¥ç±»åˆ«æ•°é‡
    print(f"\nğŸ·ï¸  é…ç½®ç±»åˆ«æ•°: {NUM_CLASSES} ({', '.join(CLASS_NAMES[:3])}...)")


# åœ¨ train_and_validate() å¼€å¤´è°ƒç”¨

def train_and_validate():
    check_dataset_integrity()

    print("ğŸš€ å‡†å¤‡è®­ç»ƒæ•°æ®...")
    train_clean = ensure_path(os.path.join(RESULTS_DIR, "dataset", "train"))
    val_clean = ensure_path(os.path.join(RESULTS_DIR, "dataset", "val"))

    shutil.rmtree(train_clean, ignore_errors=True)
    shutil.rmtree(val_clean, ignore_errors=True)

    prepare_yolo_dataset(os.path.join(ROOT_DIR, "images", "train"), train_clean)
    prepare_yolo_dataset(os.path.join(ROOT_DIR, "images", "val"), val_clean)

    print("ğŸ§  åŠ è½½ RT-DETR æ¨¡å‹...")
    model = YOLO(MODEL_NAME)
    model.model.yaml['nc'] = NUM_CLASSES  # è¦†ç›–æ¨¡å‹é…ç½®

    model.model.names = CLASS_NAMES  # è®¾ç½®ç±»åˆ«å
    # âœ… ä½¿ç”¨åŸå§‹æ•°æ®è·¯å¾„
    data_yaml = create_data_yaml(os.path.join(ROOT_DIR, "images", "train"))

    model.train(
        data=data_yaml,
        imgsz=IMG_SIZE,
        epochs=4,
        batch=BATCH_SIZE,
        name="lung_ct_rtdetr",
        device=DEVICE,
        project=RESULTS_DIR,
        exist_ok=True,
        # âœ… å…³é”®è°ƒæ•´ï¼š
        optimizer='adamw',
        lr0=0.0001,  # ä» 0.001 é™ä½ 10 å€
        lrf=0.01,  # æœ«å°¾å­¦ä¹ ç‡ = lr0 * lrf
        augment=True,
        mosaic=0.0,  # åŒ»å­¦å›¾åƒç¦ç”¨ mosaicï¼ˆä¼šç ´åç—…ç¶ï¼‰
        mixup=0.0,  # ç¦ç”¨ mixup
        flipud=0.0,  # ç¦ç”¨ä¸Šä¸‹ç¿»è½¬ï¼ˆCT æœ‰æ–¹å‘æ€§ï¼‰
        fliplr=0.5,  # ä»…ä¿ç•™å·¦å³ç¿»è½¬
        workers=0,  # Windows å¿…é¡»ä¸º 0
        patience=15,
        close_mosaic=0  # ä¸å…³é—­ mosaicï¼ˆå› ä¸ºå·²ç¦ç”¨ï¼‰
    )

    print("ğŸ” éªŒè¯æ¨¡å‹...")
    metrics = model.val(data=data_yaml, imgsz=IMG_SIZE,workers=0)  # éªŒè¯ä¹Ÿç”¨åŒä¸€ä¸ª YAML
    return metrics




def batch_predict_and_save_csv(test_images_dir):
    best_pt = os.path.join(RESULTS_DIR, "lung_ct_rtdetr", "weights", "best.pt")
    model = YOLO(best_pt)

    test_dir = Path(test_images_dir)
    save_dir = Path(RESULTS_DIR) / "predictions"
    save_dir.mkdir(parents=True, exist_ok=True)

    image_paths = []
    for ext in ["*.jpg", "*.jpeg", "*.png", "*.JPG", "*.PNG"]:
        image_paths.extend(test_dir.rglob(ext))

    results_list = []
    print(f"ğŸ¯ æ‰¹é‡é¢„æµ‹ {len(image_paths)} å¼ æµ‹è¯•å›¾åƒ...")

    for img_path in image_paths:
        try:
            results = model(str(img_path), imgsz=IMG_SIZE, conf=0.01)  # ä½é˜ˆå€¼ä¿ç•™æ›´å¤šç»“æœç”¨äºè¯„ä¼°
            boxes = results[0].boxes

            for box in boxes:
                cls_id = int(box.cls.item())
                conf = float(box.conf.item())
                xyxy = box.xyxy[0].cpu().numpy()
                x1, y1, x2, y2 = xyxy

                results_list.append({
                    "image": img_path.name,
                    "class_id": cls_id,
                    "class_name": CLASS_NAMES[cls_id],
                    "confidence": conf,
                    "x1": x1,
                    "y1": y1,
                    "x2": x2,
                    "y2": y2
                })

            # ä¿å­˜å¯è§†åŒ–ç»“æœï¼ˆé«˜é˜ˆå€¼ï¼‰
            annotated = results[0].plot(conf_thres=0.25)
            cv2.imwrite(str(save_dir / img_path.name), annotated)

        except Exception as e:
            print(f"âš ï¸ è·³è¿‡ {img_path}: {e}")

    # ä¿å­˜ CSV
    df = pd.DataFrame(results_list)
    csv_path = os.path.join(RESULTS_DIR, "results.csv")
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ“Š é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {csv_path}")
    return df

def compute_roc_data(test_images_dir, pred_df):
    """
    åŸºäºé¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾ï¼Œè®¡ç®— ROC æ‰€éœ€çš„ y_true å’Œ y_score
    è¿”å›: dict {class_id: (y_true, y_scores)}
    """
    from collections import defaultdict

    # æŒ‰å›¾åƒåˆ†ç»„é¢„æµ‹ç»“æœ
    pred_by_image = defaultdict(list)
    for _, row in pred_df.iterrows():
        pred_by_image[row["image"]].append(row)

    all_y_true = defaultdict(list)
    all_y_scores = defaultdict(list)

    test_images_dir = Path(test_images_dir)
    test_labels_dir = test_images_dir.parent.parent / "labels" / test_images_dir.name

    image_files = []
    for ext in ["*.jpg", "*.jpeg", "*.png", "*.JPG", "*.PNG"]:
        image_files.extend(test_images_dir.rglob(ext))

    for img_path in image_files:
        label_path = find_label_file(img_path)
        if not label_path or not label_path.exists():
            continue

        # è¯»å–çœŸå®æ ‡ç­¾ [class_id, cx, cy, w, h]ï¼ˆå½’ä¸€åŒ–ï¼‰
        with open(label_path, 'r', encoding='utf-8') as f:
            gt_boxes = []
            for line in f:
                parts = line.strip().split()
                if len(parts) < 5:
                    continue
                cls_id = int(float(parts[0]))
                cx, cy, w, h = map(float, parts[1:5])
                x1 = (cx - w / 2) * IMG_SIZE
                y1 = (cy - h / 2) * IMG_SIZE
                x2 = (cx + w / 2) * IMG_SIZE
                y2 = (cy + h / 2) * IMG_SIZE
                gt_boxes.append((cls_id, np.array([x1, y1, x2, y2])))

        # è·å–è¯¥å›¾çš„é¢„æµ‹
        preds = pred_by_image.get(img_path.name, [])
        pred_boxes = []
        for p in preds:
            pred_boxes.append((
                p["class_id"],
                np.array([p["x1"], p["y1"], p["x2"], p["y2"]]),
                p["confidence"]
            ))

        # å¯¹æ¯ä¸ªç±»åˆ«ç‹¬ç«‹å¤„ç†
        for cls_id in range(NUM_CLASSES):
            # å½“å‰ç±»åˆ«çš„ GT
            gt_cls = [box for cid, box in gt_boxes if cid == cls_id]
            # å½“å‰ç±»åˆ«çš„é¢„æµ‹
            pred_cls = [(box, conf) for cid, box, conf in pred_boxes if cid == cls_id]

            # æ ‡è®°æ‰€æœ‰é¢„æµ‹ä¸ºè´Ÿä¾‹ï¼ˆåˆå§‹ï¼‰
            y_true_cls = [0] * len(pred_cls)
            y_score_cls = [conf for _, conf in pred_cls]

            # å¦‚æœæœ‰ GTï¼Œå°è¯•åŒ¹é…ï¼ˆIoU >= 0.5ï¼‰
            matched_gt = set()
            if gt_cls:
                for i, (pred_box, conf) in enumerate(pred_cls):
                    best_iou = 0
                    best_j = -1
                    for j, gt_box in enumerate(gt_cls):
                        if j in matched_gt:
                            continue
                        iou = compute_iou(pred_box, gt_box)
                        if iou > best_iou:
                            best_iou = iou
                            best_j = j
                    if best_iou >= 0.5 and best_j != -1:
                        y_true_cls[i] = 1
                        matched_gt.add(best_j)

            if y_true_cls:  # é¿å…ç©ºåˆ—è¡¨
                all_y_true[cls_id].extend(y_true_cls)
                all_y_scores[cls_id].extend(y_score_cls)

    return dict(all_y_true), dict(all_y_scores)

def compute_iou(box1, box2):
    """è®¡ç®—ä¸¤ä¸ªæ¡†çš„ IoU"""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])

    inter_area = max(0, x2 - x1) * max(0, y2 - y1)
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union_area = box1_area + box2_area - inter_area

    return inter_area / union_area if union_area > 0 else 0


def generate_evaluation_report(metrics, pred_df):
    from sklearn.metrics import roc_curve, auc, precision_recall_curve, f1_score

    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    sns.set(style="whitegrid")

    # è®¡ç®— ROC/PR æ‰€éœ€æ•°æ®
    test_images_dir = os.path.join(ROOT_DIR, "images", "test")
    y_true_dict, y_score_dict = compute_roc_data(test_images_dir, pred_df)

    # åˆ›å»ºå¤šé¡µ PDFï¼ˆä½¿ç”¨ matplotlib çš„ PdfPagesï¼‰
    from matplotlib.backends.backend_pdf import PdfPages
    report_path = os.path.join(RESULTS_DIR, "evaluation_report.pdf")

    with PdfPages(report_path) as pdf:
        # =============== ç¬¬ä¸€é¡µï¼šåŸºç¡€æŒ‡æ ‡ ===============
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle("RT-DETR è‚ºéƒ¨CTç—…ç¶æ£€æµ‹è¯„ä¼°æŠ¥å‘Š - ç¬¬1é¡µ", fontsize=16)

        # 1. mAP
        map50 = metrics.box.map50
        map5095 = metrics.box.map
        axes[0, 0].bar(["mAP@0.5", "mAP@0.5:0.95"], [map50, map5095], color=['skyblue', 'salmon'])
        axes[0, 0].set_title("æ•´ä½“æ£€æµ‹æ€§èƒ½")
        for i, v in enumerate([map50, map5095]):
            axes[0, 0].text(i, v + 0.01, f"{v:.3f}", ha='center')

        # 2. å„ç±»åˆ« AP@0.5
        ap_per_class = metrics.box.ap[:, 0]
        axes[0, 1].barh(CLASS_NAMES, ap_per_class, color='lightgreen')
        axes[0, 1].set_title("å„ç±»åˆ« AP@0.5")

        # 3. ç½®ä¿¡åº¦åˆ†å¸ƒ
        axes[1, 0].hist(pred_df["confidence"], bins=30, color='orange', alpha=0.7)
        axes[1, 0].set_title("é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ")

        # 4. å„ç±»åˆ«é¢„æµ‹æ•°é‡
        class_counts = pred_df["class_name"].value_counts()
        axes[1, 1].bar(class_counts.index, class_counts.values, color='purple', alpha=0.7)
        axes[1, 1].set_title("å„ç±»åˆ«é¢„æµ‹æ•°é‡")
        axes[1, 1].tick_params(axis='x', rotation=45)

        plt.tight_layout(rect=[0, 0, 1, 0.96])
        pdf.savefig(fig)
        plt.close()

        # =============== ç¬¬äºŒé¡µï¼šROC æ›²çº¿ ===============
        fig, ax = plt.subplots(figsize=(10, 8))
        fig.suptitle("RT-DETR è‚ºéƒ¨CTç—…ç¶æ£€æµ‹è¯„ä¼°æŠ¥å‘Š - ç¬¬2é¡µï¼šROC æ›²çº¿", fontsize=16)

        macro_auc_roc = 0
        valid_classes_roc = 0

        for cls_id in range(NUM_CLASSES):
            if cls_id in y_true_dict and len(y_true_dict[cls_id]) > 0:
                y_true = y_true_dict[cls_id]
                y_score = y_score_dict[cls_id]
                if len(np.unique(y_true)) < 2:
                    continue
                fpr, tpr, _ = roc_curve(y_true, y_score)
                roc_auc = auc(fpr, tpr)
                macro_auc_roc += roc_auc
                valid_classes_roc += 1
                ax.plot(fpr, tpr, lw=1.5, alpha=0.8,
                        label=f'{CLASS_NAMES[cls_id]} (AUC={roc_auc:.2f})')

        if valid_classes_roc > 0:
            macro_auc_roc /= valid_classes_roc
            ax.plot([0, 1], [0, 1], 'k--', lw=1, label="Random")
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('False Positive Rate')
            ax.set_ylabel('True Positive Rate')
            ax.set_title(f'å„ç±»åˆ« ROC æ›²çº¿ (å®å¹³å‡ AUC = {macro_auc_roc:.3f})')
            ax.legend(loc="lower right", fontsize=8)

        plt.tight_layout(rect=[0, 0, 1, 0.96])
        pdf.savefig(fig)
        plt.close()

        # =============== ç¬¬ä¸‰é¡µï¼šPR æ›²çº¿ + F1 ===============
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))
        fig.suptitle("RT-DETR è‚ºéƒ¨CTç—…ç¶æ£€æµ‹è¯„ä¼°æŠ¥å‘Š - ç¬¬3é¡µï¼šPRæ›²çº¿ä¸F1-score", fontsize=16)

        # PR æ›²çº¿
        macro_ap_pr = 0
        valid_classes_pr = 0
        f1_scores = []

        for cls_id in range(NUM_CLASSES):
            if cls_id in y_true_dict and len(y_true_dict[cls_id]) > 0:
                y_true = y_true_dict[cls_id]
                y_score = y_score_dict[cls_id]
                if len(np.unique(y_true)) < 2:
                    f1_scores.append(0)
                    continue

                precision, recall, thresholds = precision_recall_curve(y_true, y_score)
                ap = auc(recall, precision)
                macro_ap_pr += ap
                valid_classes_pr += 1

                # è®¡ç®— F1 å¹¶æ‰¾æœ€å¤§å€¼
                f1_vals = 2 * (precision * recall) / (precision + recall + 1e-8)
                best_f1 = np.max(f1_vals)
                f1_scores.append(best_f1)

                ax1.plot(recall, precision, lw=1.5, alpha=0.8,
                         label=f'{CLASS_NAMES[cls_id]} (AP={ap:.2f})')
            else:
                f1_scores.append(0)

        if valid_classes_pr > 0:
            macro_ap_pr /= valid_classes_pr
            ax1.set_xlim([0.0, 1.0])
            ax1.set_ylim([0.0, 1.05])
            ax1.set_xlabel('Recall')
            ax1.set_ylabel('Precision')
            ax1.set_title(f'å„ç±»åˆ« PR æ›²çº¿ (å®å¹³å‡ AP = {macro_ap_pr:.3f})')
            ax1.legend(loc="lower left", fontsize=7)

        # F1-score æŸ±çŠ¶å›¾
        ax2.barh(CLASS_NAMES, f1_scores, color='coral')
        ax2.set_title("å„ç±»åˆ«æœ€ä½³ F1-score")
        ax2.set_xlabel("F1-score")

        plt.tight_layout(rect=[0, 0, 1, 0.96])
        pdf.savefig(fig)
        plt.close()

    print(f"ğŸ“ˆ è¯„ä¼°æŠ¥å‘Šï¼ˆå«ROCã€PRã€F1ï¼‰å·²ä¿å­˜è‡³: {report_path}")

def main():
    print("="*60)
    print("ğŸ¤– å…¨è‡ªåŠ¨ RT-DETR è‚ºéƒ¨CTç—…ç¶æ£€æµ‹ç³»ç»Ÿå¯åŠ¨ï¼")
    print("="*60)

    if sys.platform == "win32":
        os.environ["PYTHONIOENCODING"] = "utf-8"

    # Step 1: è®­ç»ƒ + éªŒè¯
    metrics = train_and_validate()

    # Step 2: æ‰¹é‡é¢„æµ‹æµ‹è¯•é›†
    test_images_dir = os.path.join(ROOT_DIR, "images", "test")
    pred_df = batch_predict_and_save_csv(test_images_dir)

    # Step 3: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    generate_evaluation_report(metrics, pred_df)

    print("\nğŸ‰ å…¨æµç¨‹å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœç›®å½•: {RESULTS_DIR}")
    print("   â”œâ”€â”€ predictions/       # å¸¦æ£€æµ‹æ¡†çš„å›¾åƒ")
    print("   â”œâ”€â”€ results.csv        # æ‰€æœ‰é¢„æµ‹ç»“æœï¼ˆCSVï¼‰")
    print("   â””â”€â”€ evaluation_report.pdf  # è¯„ä¼°å›¾è¡¨")

if __name__ == "__main__":
    main()